# Arguments for the constructor of ProgramGenerator and QuestionReconstructor model
model_input_size: 256
model_hidden_size: 256
model_num_layers: 2
model_dropout: 0.0

# Optimization arguments (adam optimizer by default)
optim_num_iterations: 30000
optim_weight_decay: 0.000
optim_batch_size: 256

### Learning rate scheduling - (lr *= gamma) if program generation accuracy plateaus
optim_lr_initial: 0.001
optim_lr_gamma: 0.5
optim_lr_patience: 3

### Question coding hyper parameters
qc_num_supervision: 600
# These don't mean anything when training a baseline.
qc_alpha: 1.0
qc_beta: 1.0
qc_delta: 1.0

### Optimization objective, "baseline": just train a program generator on examples with
### supervision. "ours": optimize the whole question coding objective.
qc_objective: "baseline"
qc_average_logprobs_across_timesteps: true

# Arguments for the constructor of ProgramPrior model
prior_checkpoint: "data/program_prior_best_checkpoint.pth"  # ppl: 2.600
prior_input_size: 256
prior_hidden_size: 256
prior_num_layers: 2
prior_dropout: 0.0
