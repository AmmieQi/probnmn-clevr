RANDOM_SEED: 0
PHASE: question_coding

# Training objective, "baseline": just train a program generator on examples with
# supervision. "ours": optimize the whole question coding objective.
OBJECTIVE: baseline
SUPERVISION: 1000
SUPERVISION_QUESTION_MAX_LENGTH: 40

# Arguments for the constructor of ProgramGenerator model.
PROGRAM_GENERATOR:
  INPUT_SIZE: 256
  HIDDEN_SIZE: 256
  NUM_LAYERS: 2
  DROPOUT: 0.0

# Arguments for the constructor of QuestionReconstructor model.
QUESTION_RECONSTRUCTOR:
  INPUT_SIZE: 256
  HIDDEN_SIZE: 256
  NUM_LAYERS: 2
  DROPOUT: 0.0

# ALPHA, BETA, DELTA are not used when OBJECTIVE is "baseline".
ALPHA: 1.0
BETA: 1.0
DELTA: 1.0

# Optimization arguments, we use Adam optimizer and ReduceLROnPlateau scheduler.
OPTIM:
  BATCH_SIZE: 256
  NUM_ITERATIONS: 25000
  WEIGHT_DECAY: 0.0

  # Learning rate scheduling: (lr *= gamma) if program generation accuracy plateaus.
  LR_INITIAL: 0.001
  LR_GAMMA: 0.5
  LR_PATIENCE: 3

# Path to pre-trained checkpoint of ProgramPrior.
CHECKPOINTS:
  PROGRAM_PRIOR: checkpoints/program_prior_best.pth

# Arguments for the constructor of ProgramPrior model (loaded from checkpoint).
PROGRAM_PRIOR:
  INPUT_SIZE: 256
  HIDDEN_SIZE: 256
  NUM_LAYERS: 2
  DROPOUT: 0.0
