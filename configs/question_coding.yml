# Arguments for the constructor of ProgramGenerator and QuestionReconstructor model
model_input_size: 256
model_hidden_size: 128
model_num_layers: 2
model_dropout: 0.0

# Optimization arguments (adam optimizer by default)
optim_num_iterations: 15000
optim_weight_decay: 0.000

### Batch size scheduling - (bs *= gamma) at specified iterations
optim_bs_initial: 256
optim_bs_gamma: 2
optim_bs_steps: [7000, 9000]  # batch size reachs 1024 this way

### Learning rate scheduling - (lr *= gamma) at specified iterations
optim_lr_initial: 0.01
optim_lr_gamma: 0.5
optim_lr_steps: [4000, 6000, 8000]

### Question coding hyper parameters
qc_num_supervision: 600
qc_alpha: 10.0
qc_beta: 0.1
qc_delta: 0.99

########################################################### NOT USED RIGHT NOW
### Curriculum training based on question curriculum
### (question_max_length += gamma at specified locations)
qc_length_curriculum_initial: 8
qc_length_curriculum_gamma: 5
qc_length_curriculum_steps: [250, 500, 1000, 2000, 3000, 4000, 4500, 5000]
### Maximum question length in CLEVR v1.0 is 45.
##############################################################################

### Change the optimization objective from baseline (MLE of program generation) to ours.
### This means inclusion of scaled supervision loss and reconstruction loss and ELBO.
### Set this greater than number of iterations to train only a baseline.
qc_objective_step: 6000

# Arguments for the constructor of ProgramPrior model
prior_checkpoint: "data/program_prior_best_checkpoint.pth"  # ppl: 2.600
prior_input_size: 256
prior_hidden_size: 256
prior_num_layers: 2
prior_dropout: 0.0
