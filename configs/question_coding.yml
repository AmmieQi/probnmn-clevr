# seq2seq model arguments for program generator and question reconstructor
### default: 2-layer lstm
embedding_size: 128
rnn_hidden_size: 64
rnn_dropout: 0.2

# Optimization related arguments
num_iterations: 10000
weight_decay: 0.002

### learning rate scheduling - (lr *= gamma)
initial_lr: 0.01
lr_gamma: 0.5
lr_steps: [4000, 6000, 8000]

### batch size scheduling - (bs *= gamma) at specified iterations
initial_bs: 256
bs_gamma: 2
bs_steps: [7000, 9000]  # batch size reachs 1024 this way

### question coding: data loading
num_supervision: 600

### question coding: objective
ssl_alpha: 10.0
kl_beta: 0.1
elbo_delta: 0.99

### Change the optimization objective from 'baseline' to ours.
### This means inclusion of scaled supervision loss and reconstruction loss and ELBO.
### Set this greater than number of iterations to train only a baseline.
elbo_turnon_step: 3000

# Arguments for the constructor of ProgramPrior model
prior_checkpoint: "data/program_prior_best_checkpoint.pth"  # ppl: 2.596
prior_input_size: 256
prior_hidden_size: 128
prior_num_layers: 4
prior_dropout: 0.0
